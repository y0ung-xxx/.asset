？？？许多不足。

对什么？？此，本文提出一种结合多目标检测与跟踪、人体姿态识别和卷积神经网络的**学生课堂行为判定算法**，能够在真实课堂环境下判定学生行为，并计算课堂质量评估相关指标。 本文具体研究工作如下：

（1）针对目前缺少公开的学生课堂行为数据集的问题，

本文构建了包含“听课”，“低头”，“举手”和“起立”4种常见学生课堂行为，共3532张图像实例的数据集。真实环境下的学生课堂行为数据集RSCBD。

（2）经过对比实验选择了性能更佳的YOLO V5结合 Deepsort多目标跟踪算法作为系统搭载的模型。

（3）基于人体姿态识别和卷积神经网络的学生行为判定算法。使用Alphapose模型处 理数据集，VGG16网络构建学生行为判定分类器，在RSCBD数据集上进行了训练和验证， 通过对比实验证明，本研究提出的算法相比仅用卷积神经网络有明显的优势。

（4）基于学生课堂行为的课堂质量评估指标设计。根据常见学生课堂行为，对课堂质量评估体系进行了需求分析，总结出实时抬头率、实时低头率、课堂活跃度以及举手次数4项指标并实现了计算

通过测试，本文提出的学生课堂行为判定算法在验证集上最高准确率能够达到96.05%， 同时能够在真实课堂视频环境下完成对所设计指标的计算。

但仍需要在运行速度和实时性 上进一步改进和提高。

# **第一章绪论**

1.  研究背景和意义

这种方法完全依赖人类观察者，不仅费时费力，而且可能掺杂主观因素，难以客观、全面地反映课堂教学情况。

利用深度学习、计算机视觉等技术智能地判定学生课堂行为，对课堂教学情况进行分析，进而提高课堂质量，具有非常重要的意义。

1.  研究现状

基于编码系统的课堂教学视频分析已经被广泛承认为一种行之有效的方法，典型的有S-T课 堂教学分析[法［2］、](#bookmark176)弗兰德斯互动分析系[统［3］、](#bookmark177)Transana平台［4］等。这种方法完成了一部分本 需要人工进行的工作，但仍存在一些不足：如需要分析者具有较高的编码相关领域知识， 本质上还是依赖人类自身对课堂视频进行分析等。

近年来，深度学习及计算机视觉等技术快速发展，一些研究者开始研究将其应用在课堂视频分析上的可能性。

深度学习对高质量数据集的依赖度很高，Bo Su[n［5］等](#bookmark179)人收集了 128 个不同学科和11个课程的视频，形成了包含检测部分、识别部分和字幕部分的样本，共包

含超过15000个样本，并且通过实验证明了其可靠性。

周叶［6］以及谭斌［7］基于Faster R-CNN 目标检测模型实现了对静态图像和动态视频中小学生课堂行为的识别。蒋沁沂［8］等提出使 用基于残差结构的深度残差网络来解决目前卷积神经网络随网络层数加深性能退化的问题，能够识别出上课、睡觉、东张西望等行为。廖鹏［9］等人基于VGG预训练网络模型进行迁移学习，来提取学生课堂异常行为特征，实现了对玩手机、睡觉等异常行为的检测分析。 朱朝［10］结合两阶段目标检测模型和深度残差网络，实现了对抬头低头行为的识别和统计， 并能够针对单个学生进行状态分析。徐家臻［11］，杨[凡［12］，](#bookmark186)黄[冠［13］，](#bookmark187)高科[威［14］，](#bookmark188)林灿[然［15］](#bookmark189) 等研究者尝试将人体姿态识别技术融合进学生课堂行为方法中，在镜头距离近，清晰度高的理想环境下取得了较好的准确率，但在真实课堂环境下准确率与理想环境差距较大。

1.  本文研究内容

针对以上问题，本文提出一种结合多目标检测与跟踪、人体姿态识别和深度卷积神经 网络的学生课堂行为判定算法，能够在真实课堂环境下判定学生行为，并计算课堂质量评 估相关指标。具体研究工作如下：

（1）真实环境下的学生课堂行为数据集（Real-Environment Student's Classroom Behaviors Dataset，以下简称RSCBD）。针对目前缺少公开的学生课堂行为数据集的问题， 从互联网上搜集、下载并截取了3532张学生课堂行为图像实例，其中训练集3076张，验 证集456张，包含“听课”，“低头”，“举手”和“起立”4种常见学生课堂行为。该数据 集用于本研究其他实验。

（2）基于深度学习的目标检测及跟踪模块。研究并介绍了一阶段目标检测模型YOLO V5和两阶段目标检测模型Faster R-CNN，经过对比实验选择了性能更佳的YOLO V5作为 系统搭载的模型。同时研究了 Deepsort多目标跟踪算法，将其与YOLO V5模型结合，作 为课堂质量评估指标计算的基础。

（3）基于人体姿态识别和卷积神经网络的学生行为判定算法。研究并介绍典型的自 底向上姿态估计框架Openpose和自顶向下姿态估计框架Alphapose，根据在学生课堂视频 上的姿态提取效果选择使用Alphapose模型处理数据集。研究并介绍VGG16网络，并使用 该网络构建学生行为判定分类器，在RSCBD数据集上进行了训练和验证，通过对比实验 证明，本研究提出的算法相比仅用卷积神经网络有明显的优势。

（4）基于学生课堂行为的课堂质量评估指标设计。根据常见学生课堂行为，对课堂

质量评估体系进行了需求分析，总结出实时抬头率、实时低头率、课堂活跃度以及举手次 数4项指标并实现了计算。

1.4论文组织结构

本文共分七个章节，具体组织结构如下：

第一章主要介绍通过课堂视频判定分析学生课堂行为的研究背景、现状和意义；

第二章介绍与本研究相关的基础知识和技术，包括多目标检测技术、多目标跟踪技术、 人体姿态识别技术和卷积神经网络等，并对两种典型的人体姿态识别技术进行了对比和选 择；

第三章介绍构建真实环境下的学生课堂行为数据集相关工作，作为后续章节的实验的 基础；

第四章介绍多目标检测及跟踪模块，对两种典型的目标检测模型进行对比和选择，同 时研究多目标跟踪经典算法，展示检测及跟踪效果；

第五章提出基于人体姿态识别和卷积神经网络的学生行为判定算法，给出算法流程并 通过实验证明算法优越性；

第六章探讨学生行为判定在课堂质量评估中的应用，设计指标并进行计算；

第七章总结全文工作，并指出不足和未来改进的方向。

第二章相关理论及技术基础

本章将详细介绍本研究所涉及各项基本理论及技术原理，主要包括多目标检测及跟踪 技术、人体姿态识别技术和深度学习技术。

1.  多目标检测技术

目标检测是计算机视觉领域的重要基本问题之一，也是许多其他计算机视觉研究问题 的基础。近年来深度学习技术的迅速发展，为目标检测注入了新的活力，数年内就取得了 显著的突破，将其推向了一个前所未有的研究热点。许多研究发现，深度学习提取的特征 信息在目标检测领域性能优势显著，有着很强的表现力和鲁棒性。目前已产生了许多监测 效果优秀的算法，大致可分为如下两类：

1.  基于回归的“一阶段”目标检测算法，检测速度相对更快，一般步骤是特征提取、 分类和定位回归。
2.  基于候选框生成和分类的“两阶段”目标检测算法，特点是较丰富的特征和较高的 准确率，一般步骤是特征抽取、生成候选框、分类和定位回归。
    1.  “一阶段”目标检测算法

在“一阶段”目标检测算法中，比较著名的有YOLO系列算法［16］和SSD算法［17］。YOLO 算法优点是在训练阶段使用P-ReLU激活函数，检测速度快，能够达到实时性要求；其缺 点是只分析最后7\*7大小的特征图谱，因此对小目标的检测质量不佳，难以区分多个目标 在同一个网络单元的情况。SSD算法与YOLO算法相比，加入了多尺度特征检测和匹配策 略，同时修改VGG16结构并加入了 atrous算法，以解决YOLO算法定位精度较低的问题。 该算法的优点是定位准确和算法速度快；而缺点是小目标的特征模糊不利于检测，且存在 没有候选区域时，难以回归，容易导致不收敛的问题。

1.  “两阶段”目标检测算法

在“两阶段”目标检测算法中，以最早的R-CNN算法［18］为基础，不断改进出Fast R-CNN 算法［19］，Faster R-CNN算法［20］。R-CNN算法将大规模的卷积神经网络应用在自下而上的 候选区域来定位和分割对象，以及当标记的训练集规模不足时，对特定任务进行优化，提 高模型性能。该算法那缺点在于重复计算量大，且训练测试复杂，候选区域获取、特征获 取、分类和回归都是单独运行的，使得检测速度缓慢。Fast R-CNN算法在此基础上改进了

ROIpooling层，减少了大量特征提取过程中产生的冗余，使训练速度和测试速度都得到了 提高。但其预测框生成算法使用的还是选择性搜索(Selective Search)算法，注定其速度 不可能提到很快。而Faster R-CNN添加了预测框生成网络RPN，目标检测所需要的四个 步骤：候选区域生成，特征提取，分类器分类，回归器回归，全都交由深度神经网络在 GPU 上进行处理，大大提高了操作的效率。

早期YOLO系列模型的检测精度较“两阶段”模型有一定差距，但其更新换代速度极 快，如今已迭代更新出YOLO V5模型。最新的YOLO V5模型无论在速度还是精度上都已 经远超“两阶段”模型。本文将在后续章节具体介绍YOLO V5模型及Faster R-CNN的结 构及特点。

1.  多目标跟踪技术

多目标跟踪(Multiple Object Tracking，MOT)是在目标数量非已知的前提下，对视频中 多个目标进行检测、赋予身份(ID)及轨迹跟踪的技术。不同的目标绑定不同的ID，能够 实现对单个目标的连续性分析，为后续轨迹预测、精准查找等工作打下基础。

目前最为成熟的多目标跟踪方案为基于检测进行跟踪的方案，也是当前多目标跟踪的 主流框架，该框架近几年极大地推动了 MOT任务的前进。代表算法有Sort算法、Deepsort 算法等。算法框架如图2.1所示：![](media/b157a89b7588cdae4291471a091e50a6.jpeg)

图2.1 基于检测的跟踪方案框架

1.  Sort 算法

Sort算法由 Alex Bewle[y［21］等](#bookmark195)人于2016年提出，原论文名为"Simple Online and Realtime Tracking"，以其简单而高效的特点著称。尽管只是将卡尔曼滤波(Kalman Filter)和匈牙 利算法(Hungarian algorithm)这样的普通算法进行结合,其性能却可以与SOTA算法［22］相当， 且速度比后者快20倍。其核心模块包含预测模型和数据关联。

(1)**预测模型。**对每个检测目标状态进行建模，用于传播到下一帧比较处理。每个目标 的状态模型如公式2.1所示。其中u、v表示目标中心的x、y坐标，s、r表示预测 框的尺寸(面积)和长宽比，优仇6表示下一帧的预测中心的坐标和检测框面积。预 测框参数用于更新目标状态，其中的速度分量使用卡尔曼滤波进行求解。

*X* = *[u, v, s, r,*",**仇** *s']T* (2.1)

1.  **数据关联。**为现有目标分配检测框时，每个目标的边界框形状通过预测其在当前帧 中的新位置进行估计。然后采用匈牙利算法计算分配代价矩阵，将其作为目标与检 测框之间的交并比(IOU)。如果IOU低于阈值就不分配检测框。使用IOU的优点是 可以隐式解决目标的短期遮挡问题，即当目标被遮挡对象覆盖时，只检测遮挡对象， 被覆盖目标不会受到影响。
    1.  Deepsort 算法

Deepsort算法[23]是对Sort算法的改进，总体思路延续了 Sort算法的框架：卡尔曼滤波 加匈牙利算法。在此基础上，增加了深度关联特征(Deep Association Metric)。其特点是能 够获取外观特征信息并加入帧间计算中，保留已删除ID的目标的外观特征，在其重新加 入图像时“回忆”起原ID，降低了 Sort算法的ID切换频率。

1.  人体姿态识别技术

人体姿态识别是利用深度学习技术提取人体骨骼关键点，并通过各关键点的位置关系 描述人体骨架形态的技术，对于描述、分析、预测人体姿态和行为具有重要意义。目前基 于深度学习的多人人体姿态识别技术主要分为两大类：自底向上的方法和自顶向下的方法。 自底向上的方法指先检测出人体关键点，再根据相互位置关系以及图论等规则将分散的关 键点连接成完整的人体，其代表模型为Openpos[e[24]。](#bookmark198)自顶向下的方法则先检测出人体，生 成人体预测框，再送入关键点提取算法中获取骨骼信息，连接并最终映射回原图，其代表 模型为 Alphapos[e[25]。](#bookmark199)

1.  自底向上方法：Openpose

Openpose以caffe为框架，是基于卷积神经网络和有监督学习的开源库，其识别速度 较快，适用于单人和多人场景，是第一个提出的基于深度学习的人体姿态实时检测模型。 其算法流程如下：先将图片输入VGG19网络提取特征，再将得到的图像特征送入关键点

预测网络得到关键点的坐标、置信度和亲和度，最后利用置信度向量和亲和度向量时间关 键点的聚类和组装［26］。其算法流程如图2.2所示：

![](media/8354478a5bda1f177616ec5c892852ab.jpeg)

（a）输入图像 （b）预测并二分匹配 （c）估计结果

图2. 2 Openpose算法处理过程

1.  自顶向下方法：Alphapose

Alphapose最早的版本基于pytorch框架，发布团队基于该模型的基础框架后续提出了 区域多人姿态估计框架RMPE，能够实时进行多人姿态估计并且准确度较高。其算法流程 如图2.3所示。其具体算法流程及特点将在本文后续章节中介绍。

![](media/15503ea858612b86a5f7c6aeb15ac082.jpeg)

图2. 3 Alphapose算法处理过程

1.  分析比较

两种模型在处理图像上各有优势，自底向上的模型Openpose处理图像速度相对更快， 而自顶向下的模型Alphapose检测关键点准确率相对更高。由于大多数课堂监控录像视频 清晰度不会太高，因此我们将优先考虑准确率因素。本研究针对真实课堂中截取的部分图 像，使用两种人体姿态识别模型分别进行了估计测试，检测对比结果如图2.4所示。

检测对比结果显示，在对学生图像进行处理时，Openpose完全没有识别到“听课”学 生的关键点，而Alphapose准确识别出了多个关键点。因此本文将选择Alphapose作为学 生骨架提取的模型。

![](media/89ff7bf9cda39d44caf66e4467ae4097.jpeg)

（a）原始图像 （b）Openpose处理结果 （c）Alphapose处理结果

图2.4 Openpose与Alphapose处理结果对比

1.  卷积神经网络

卷积神经网络（Convolutional Neural Networks, CNN）是一种包含卷积计算的多层神 经网络。其在计算机视觉领域的大放异彩源自2012年的大规模图像识别竞赛（ImageNet Large Scale Visual Recognition Challenge，ILSVRC），深度神经网络模型 AlexNe[t[27]在](#bookmark201)当年 夺得第一。由此，越来越多的研究者投入到了网络结构的研究当中，产生了许多层数更深、 特征提取能力更强的网络结构:例如提出于2013年的ZFNe[t[28]，](#bookmark202)提出于2014年的VGGNe[t[29]](#bookmark203) 和GoogleNe[t[30]，](#bookmark204)提出于2015年的ResNe[t[31]等](#bookmark205)。其中ResNet在竞赛中已经能够取得96.5% 的识别准确率，高于人类所能达到的水平。这说明在某些领域的发展水平上，人工智能已 经达到甚至超越了真人。

1.  经典卷积神经网络结构

卷积神经网络被广泛认为在图像识别和分类领域有显著的效果，通过对图像数组进行 特征提取并不断学习，能够识别复杂的图像属性。经典卷积神经网络一般包含卷积层、池 化层和全连接层结构。下面将对这些层结构进行详细介绍。

1.  **卷积层。**是卷积神经网络中最基础、最重要的结构，用于执行卷积操作。卷积操作 指用一定大小的窗口（即卷积核）在图像上进行滑动，来提取图像特征，如图 2.5 所示。在CNN中，卷积层的输入输出数据有时被称为特征图（feature map）。前几 层可能会提取到垂直、水平边缘等低阶特征，而在随后的层中将这些低阶特征进行 组合，得到高阶特征。这些高阶特征将成为最后进行分类的依据。卷积层的结构包 含4个超参数：卷积核大小、卷积核数量、步长和补零与否。一般来说，仅使用单 个卷积核无法充分地提取特征，因此可以通过使用多个不同的卷积核来提取多重特 征。步长指的是卷积核一次异动的长度，补零则用于修正图像边界，使之与卷积核 大小吻合。

图2.5 卷积操作示意图

1.  **池化层。**又称为子采样层，通过减少数据的长宽空间来减小特征维数。一般分为最

大池化和平均池化。最大池化指在特征图某个区域中取最大值构成新特征图，平均

池化指对特征图某个区域求均值构成新特征图，如图2.6所示。池化层本质是降采

样，能够减少网络参数的同时保留有效特征，一定程度上也能阻止过拟合现象。

（a）最大池化

图2.6 池化操作示意图

1.  **全连接层。**一般处于卷积神经网络的末端，将此前层输出的二维特征图转化成一维 向量。其特点是上一层的每个神经元都与下一层的每个神经元连接。全连接层在卷 积神经网络中的作用一般是分类器，最终输出一维向量对应每个类别的置信度。其 结构如图2.7所示。

![](media/9d4433690aaf459f6303024d59a541d4.jpeg)

图2.7 全连接层示意图

2.4.2激活函数

激活函数的作用是处理上层神经元传递过来的输出，将结果继续传递到下层神经元。 由于卷积操作相当于矩阵相乘，是一种线性操作，在增加网络深度的过程中，如果只是对 卷积操作进行叠加，就算叠加了若干层，仍然只能拟合线性函数。并且许多实际问题并不 能使用简单的线性表达式来解决。激活函数就是用来增加网络的非线性表示，使得神经网 络拥有能拟合所有非线性函数的能力。下面将介绍四种常用的激活函数。

1.  Sigmoid 函数。

Sigmoid函数又被称为Logistic函数，取值范围为0到1,用于隐层神经元输出，是传 统神经网络中最常见的激活函数之一。其计算方式如公式2.2所示，函数图形是一条“S” 型曲线，如图2.8所示。Sigmoid函数的缺点在于，当输入值绝对值较大的时候梯度很小， 更新缓慢。

1

*h(x) =* (2.2)

1 + exp(一%)

![](media/02ccec5bd418b157420f5878ed3d0a6b.jpeg)

图2.8 Sigmoid函数图形

1.  ReLU 函数

考虑Sigmoid函数的上述缺点，现代网络中最常用的激活函数是ReLU函数，其具有 梯度求解简单、收敛速度快的优点，且不会在x正半轴出现梯度饱和、梯度消失等问题。 其计算方式如公式2.3所示，函数图形如图2.9所示。ReLU函数的缺点是当x处于负半轴 时，梯度为0，激活函数对数据没有响应，失去参数更新能力，也称为“坏死”。但这种 情况出现频率较低。

![](media/945ec27a075b6298c25630edc3d95b27.jpeg)

1.  P-ReLU 函数

P-ReLU函数是对ReLU函数的改进，即带参数的ReLU函数。当x值处于负半轴时，

函数值并不直接归零，而是变为带一个微小通道权重的值。在计算量几乎未增加的情况下，

P-ReLU函数能有效改善模型的过拟合问题，避免“坏死”情况，其函数图形如图2.10。

(2.4)

![](media/bd9de14de3efcfe049c889f7fd65f02d.jpeg)

图2. 10 P-ReLU函数图形

1.  Softmax 函数

Softmax函数又被称为归一化函数，是二分类激活函数Sigmoid函数在多分类上的推 广，能够将多分类的结果以概率的形式展现出来。因为Softmax函数考虑了所有分类的归 一化值，因此常被放在神经网络的最后一层，用来输出分类结果。其计算方式如公式2.5 所示。

*Softmax(zt*) = j ez° (2苫)

2.5本章小结

本章介绍了本研究所需的基础知识，并对需要用到的技术进行了概述，包括多目标检 测技术、多目标跟踪技术、人体姿态识别技术、卷积神经网络等。特别的，对两种典型的 人体姿态识别模型进行了对比测试和选择，作为后续章节研究展开的基础。

第三章真实环境下的学生课堂行为数据集

1.  数据集构建

学生课堂行为数据集的数据来源可以是实地拍摄，也可以是互联网公开课视频。因为

疫情原因，本研究未能够实地录制课堂视频，因此选择对互联网公开课视频进行截取和筛 选，以获得图像数据。

1.  数据集图像选取标准

考虑到数据集需要送入神经网络进行特征提取和学习，本研究需要对加入数据集的图 像进行筛选。图3.1展示了符合选取标准的真实课堂图像示例，将经过目标截取处理后加 入数据集。选取标准如下：

1.  摄像头位于教室正中间黑板上方，即能够保证教室中间的前排学生为正面视角。
2.  教室学生排数不超过6排，即大多数学生不会被严重遮挡。
3.  画质相对清晰，能人工识别出学生行为，便于后续进行行为标注。

![](media/af72062008ac71161729370860b05118.jpeg)![](media/aebdfdd00a34b13baeb68b95831c521c.jpeg)

图3.1 符合标准的图像示例

由于数据来源为互联网公开课视频，因此所收集视频中几乎没有出现“睡觉”、“玩手 机”等负面动作类别，同时听课、低头为出现频率最高的课堂行为，举手、起立为最常见 互动行为。考虑到数据集的规模及实例选取的难易程度，本研究选择听课、低头、举手和 起立4个动作分类来构建真实环境下的学生课堂行为数据集（Real-Environment Student's Classroom Behaviors Dataset，以下简称RSCBD）。对各个学生课堂行为的选取标准描述如 表3.2及图3.2所示：

听课 学生头部抬起，视线大致朝向摄像头（黑板），能观测到学生双眼

低头 学生头部低下，视线朝向课桌，可能无法观测到学生双眼

举手 学生手部举起，包括停在头侧及举过头顶

起立 学生手臂自然下垂，可以基本观测到完整上半身，可能观测到腿部

![](media/71a00fef9935d064cb0e2bf6bcff0d34.jpeg)![](media/37930e93eb553b6ed8b6cd2972691908.jpeg)![](media/d7d6889642d8c0292d8c7a65bef435ac.jpeg)

图3.2 学生课堂行为选取标准示例

1.  数据不平衡处理方案

在真实的课堂环境中，存在各种学生行为出现频率不均匀的问题，如“听课”行为出 现频率最高，而“举手”、“起立”的互动类行为相对较少。在本研究收集到的视频及图像 数据中，同样存在各种行为极度不平衡的情况。对此本研究采取水平翻转方式对占比少的 “举手”和“起立”行为图像进行过采样，如图3.3所示：

![](media/e58bb7d4db1bb6dd68f63cf3af37146e.jpeg)

1.  数据集特点

本研究所构建的真实环境下的学生课堂行为数据集共包含3532张图像，分为训练集 和验证集。训练集包含从36段视频中截取的3076张图像，其中“听课”行为1142张，“低 头”行为713张，“举手”行为659张，“起立”行为562张。验证集来源视频为不包含在 训练集中的9段独立视频，共有456张图像，其中“听课”行为239张，“低头”行为82 张，“举手”行为86张，“起立”行为49张。本数据集还具有如下一些特点：

1.  **真实环境。**每张学生课堂行为图像都采集于真实的课堂录像。
2.  **单目标。**每张学生课堂行为图像仅包含一名学生的行为特征。
3.  **多尺度。**学生所在教室位置不同，距离录制摄像头远近不同，前排学生截取图像的 尺度会较后排学生更大，如图3.4所示。RSCBD数据集对前后排学生、不同尺度的 图像都有囊括。

![](media/8e7b20bd091a04a5dcf39e85350b2db9.jpeg)

图3.4 多尺度采样示意图

3.3本章小结

本章主要介绍了与构建真实环境下的学生课堂行为数据集相关的工作。先介绍了国内 外现有的行为数据集，着重对与学生课堂行为有关的数据集进行了列举和归纳分析；再对 本研究所构建的数据集的数据来源、选取标准、数据处理方案和特点进行了详细阐述。本 章所构建的数据集将用于后续章节的实验。

第四章多目标检测及跟踪模块

本章将介绍典型的“一阶段”目标检测模型YOLO V5和“两阶段”目标检测模型Faster R-CNN的结构及特点，并在VOC数据集上进行“person”类检测的对比实验，并展示结 合YOLO V5模型和Deepsort算法的多目标检测及跟踪效果。

1.  YOLO V5目标检测模型

YOLO系列模型是经典的“单阶段”目标检测模型，得名于“You Only Look Once”， 依靠高准确性和实时性在众多领域得到广泛研究和应用。YOLO V5是该模型的第五代迭 代，其具体结构如图4.1所示：

![](media/f2289f28288ad3c65165ca694eea542d.jpeg)

图4.1 YOLO V5网络结构

YOLO V5中部分核心及创新组件介绍如下：

（1）Focus结构。该结构将输入的图片进行切片操作，如图4.2所示，每隔一个像素取一

次值，能够扩大通道数，使之变为原来的四倍，能够让后续特征提取更充分。

| 1 | 2 | 1 | 2 |
|---|---|---|---|
| 3 | 4 | 3 | 4 |
| 1 | 2 | 1 | 2 |
| 3 | 4 | 3 | 4 |

图4.2 切片操作

1.  CSP结构。该结构通过将浅层的特征图在通道维度一分为二，一部分进行特征提取， 另一部分则经过跨阶段层次结构与特征提取模块的输出进行合并，在准确性不变甚 至提高的基础上可以减少10%\~20%的网络参数量。在YOLO V4网络中，仅有主干 网络中使用到了 CSP结构，而YOLO V5模型设计了两种不同的CSP结构，分别应 用于Backbone网络和Neck网络。

YOLO V5是YOLO系列的第5代模型，分为四种量级的网络结构：s，m，1，x。其 网络深度及参数量依次递增。如表4.1所示。由于l模型与x模型在对person类的识别准 确率上差距不大，而参数量少接近一半，综合考虑参数量和识别准确度，本研究选择 Yolov5_l模型进行后续实验。

表4.1 YOLO V5各模型参数量及准确率对比

| 模型名称 | 参数量(M) | AP（person） |
|----------|-----------|--------------|
| Yolov5_s | 7.2       | 87.98%       |
| Yolov5_m | 21.2      | 91.69%       |
| Yolov5_l | 46.5      | 93.80%       |
| Yolov5_x | 86.7      | 94.61%       |

1.  Faster R-CNN目标检测模型

Faster R-CNN模型是经典的“两阶段”目标检测模型，其结构主要可分为四大模块: 特征提取模块(Conv Layers)，预测框生成模块(Region Proposal Network)，特征图池化 模块(RoI Pooling)和分类回归模块(Classifier)。其模型结构如图4.3所示：

![](media/06883f91638f0f91f5b6023c170d542e.jpeg)

图4.3 Faster R-CNN模型结构

(1)**特征提取模块(Conv Layers)。**使用CNN网络提取目标图像的特征，得到feature map。 可选的CNN主干网络主要有VGG16和ResNet50。

(2)**预测框生成模块(Region Proposal Network)。**对于预测框的生成，传统的滑动窗 口方法和R-CNN使用的SS(Selective Search)方法存在耗时过长的问题，Faster R-CNN直接使用RPN网络生成预测框。RPN网络能够在特征图上定位候选目标， 使用Softmax分类器将候选目标分为前景和背景，并通过并行工作的预测框回归器 修正目标位置。

(3)**特征图池化模块(RoI Pooling)。**该模块连接RPN和Classifier，其作用是将RPN 输出的不同大小的预测框映射到feature map上，通过max-pooling操作进行尺寸变 换，使每个预测框的局部feature map的输出大小一致，再送入Classifier进行多分 类别区分。

(4)**分类回归模块(Classifier)。**利用之前模块生成的预测区域feature map,判定该目 标的类别。

1.  目标检测模型对比实验
    1.  实验条件

        对比实验所使用的YOLO V5和Faster R-CNN模型都在Windows 11操作系统下搭建， 使用编程语言为Python。使用VOC数据集进行“person”单分类的训练和测试，训练集与 测试集划分比例为9:1。

        1.  衡量标准

            考虑到本算法的应用环境仅仅为课堂教学视频,不会出现太多的目标类别检测需求, 因此在正确率率方面，仅取对person类识别精确率的作为衡量指标，记为AP(person)。 在效率方面，取模型每秒处理视频图像帧的张数fps(frame per second)作为识别速度的衡 量指标。

        2.  实验结果及分析

            采用迁移学习方法，在YOLO V5模型上训练VOC数据集时，使用已经在COCO数 据集上训练好的yolov5_x预训练模型；在Faster R-CNN模型上训练VOC数据集时，使用 已经在COCO数据集上训练好的fasterrcnn_resnet50_fpn_coco预训练模型。训练轮数都为 100轮,包含10轮冻结训练和90轮解冻训练。实验结果对比如表4.2所示：

| 表4.2目标检测模型对比实验结果 |          |            |       |
|-------------------------------|----------|------------|-------|
| 模型                          | 输入大小 | AP(person) | Fps   |
| YOLO V5_L                     | 640\*640 | 93.80%     | 28.69 |
| Faster R-CNN                  | 640\*640 | 89.73%     | 10.30 |

从准确率上来看，YOLO V5_L模型识别person类别的准确率比Faster R-CNN模型高 4.07%；从识别速度上来看，YOLO V5_L模型在1秒内处理图片的张数是Faster R-CNN 模型的2.8倍。综合两项实验结果分析，YOLO V5模型在VOC数据集上的“person”类 检测效果比Faster R-CNN模型要好。

1.  目标检测及跟踪效果

经过本章对比实验，本研究将在YOLO V5目标检测模型基础上，加入多目标跟踪算 法Deepsort算法。处理流程为，对视频画面帧进行多目标检测，对于每个检测到的学生目 标，生成对应的预测框位置和置信度，传递给Deepsort算法进行处理。Deepsort算法根据 YOLO V5目标检测模型传递来的预测框位置和置信度，结合此前帧对当前帧预测的track 信息，通过匈牙利算法和卡尔曼滤波，更新相应的跟踪参数。最终达到对画面中的每个检 测目标赋予ID的功能。整体架构如图4.4所示。

![](media/009b997bdfd4d7f614393ff410b64ae5.jpeg)

图4.4 目标检测及跟踪框架

结合后的模型对课堂视频的处理效果如图4.5所示。可以看出，该模型能够完成对画 面中学生目标的检测工作，同时对每个学生目标赋予了对应的ID值，为后续进行课堂指 标计算的任务打下了基础。

![](media/8dc3e6884b883f3d648a912bfae3ccfa.jpeg)

图 4.5 YOLO V5+Deepsort 效果图

1.  本章小结

本章详细介绍了两种典型的目标检测模型YOLO V5和Faster R-CNN,并通过对比实 验，综合考虑准确率与处理速度，选择了性能更好的YOLO V5模型，结合Deepsort算法 实现了对课堂视频中学生目标的检测及跟踪，并在最后对模型检测及跟踪效果进行了展示。

第五章学生课堂行为判定算法

本章将介绍基于人体姿态识别和神经网络的学生课堂行为判定算法。首先介绍将算法 总体流程，其次详细介绍通过Alphapose提取学生课堂行为图像骨架并处理的过程，然后 将处理后的数据集输入多种神经网络进行训练并对比。

1.  算法流程

本研究结合人体姿态识别技术与神经网络分类器进行学生课堂行为的判定，总体算法 流程如图5.1所示。

![](media/ffb5702d8792ebd3d643de1b77c5381a.jpeg)

原始图像 目标检测并截取 提取骨架并处理 行为判定

图5.1 学生课堂行为判定算法流程

1.  **目标检测及裁剪。**使用第四章介绍的目标检测模型对真实课堂教学视频帧进行目标 检测并裁剪下来。
2.  **提取骨架并处理。**使用本章将介绍的Alphapose模型对截取的单人学生行为图像进 行骨架提取。同时为了排除学生体型、环境光线、衣服颜色等与课堂行为判定无关 的冗余信息，将提取出的骨架信息绘制在同等大小的纯黑色背景图像上。
3.  **行为判定。**将处理后的单人学生行为图像输入训练好的神经网络分类器进行分类， 判断样本所属的学生课堂行为类别。
    1.  基于Alphapose的人体姿态识别
        1.  Alphapose 模型介绍

Alphapose人体姿态识别模型由上海交通大学卢策吾团队于2018年提出，是一种区域 多人姿态识别模型。Alphapose采用自顶向下的方法进行人体姿态识别，根据目标检测结 果确定的候选框进行单人姿态识别。其姿态识别模块由空间变换网络（Spatial Transformer Network， STN），单人姿态估计器（Single Person Pose Estimator， SPPE），空间反变换网络 （Spatial De-Transformer Network，SDTN）和姿态非极大值抑制（Pose NonMaximum-Supp-

ression , Pose NMS）四部分组成，如图5.2所示:

![](media/942cb2c5621c65c2ecf318d596500b5f.jpeg)

图5.2 Alphapose姿态识别模块

**（1）空间变换网络STN。**STN是为传统CNN设计的显示的图像变换处理模块，能够对 输入图像实现平移、缩放、旋转、剪切变换，在与CNN 一同训练的过程中提高对 检测目标的定位准确程度，使人体尽可能处于检测框的中央。

**（2）单人姿态估计器SPPE。**采用Jia Deng等人于2016年提出的Hourglass算法［32］进行 单人姿态提取，该算法因网络结构形似沙漏（Hourglass）而得名，在当年MPII姿 态分析竞赛中位列第一，也是近几年姿态识别领域研究的主干网络。

**（3）空间反变换网络SDTN。**SDTN为STN的逆映射，功能是将SPPE估计出的人体姿 态数据映射回原始图片坐标系下。

**（4）姿态非极大值抑制Pose NMS。**Pose NMS用于精炼预测结果。对图像中的同一个对 象，目标检测模型可能会产生多个距离较近，而大小不同的预测框。非极大值抑制 对产生的交并比（IoU）较高，即重合度较高的多个预测框及其结果进行对比，选 取置信度最高的预测结果，丢弃其他预测结果。

1.  使用Alphapose进行人体姿态识别

本研究使用Windows环境下的Alphapose对RSCBD数据集中的图片逐个进行了骨架 提取，图像数据处理过程如图5.3所示：

![](media/5987caf98fd439e07a51a4f6c06eeccb.jpeg)

图5.3提取骨架及处理示意图

特别的，考虑到诸如学生体型、环境光线、衣服颜色等因素与学生课堂行为没有直接 关联，属于冗余信息，去除掉此类信息有利于神经网络进行特征提取，因此本研究仅保留 学生骨架信息。

1.  基于卷积神经网络的学生课堂行为判定分类器

卷积神经网络在处理诸如视频、音频、图像、语言文字等相互位置有关联的数据时具 有非常好的效果。本算法选择经典卷积神经网络VGG16构建学生课堂行为判定分类器。

1.  经典卷积神经网络VGG16

VGGNe[t[29]由](#bookmark203)牛津大学Visual Geometry Group在2014年提出，能够很好的完成定位和 分类任务。VGGNet共有A，A-LRN，B，C，D，E六种结构配置方案，其中VGG16对应 配置D，具体结构如图5.4所示：

**224x224x3**

**224\*224x64**

![](media/20d285ecab2438b9377c8e425d6ff606.jpeg)

图5.4 VGG16网络结构图

VGG16共包含13个卷积层（Convolutional Layer，图中表示为Conv），3个全连接层 （Fully connected Layer，图中表示为FC）和5个池化层（Pool layer，图中表示为MaxPool）。

卷积层中第一、二层使用64个3\*3卷积核，第三、四层使用128个3\*3卷积核，第五、 六、七层使用256个3\*3卷积核，其余层都使用512个3\*3卷积核。全连接层共3层，第 一、二层有4096个神经元。第三层有1000个神经元。其中，卷积层和全连接层由于具有 权重系数，也被称为权重层，共有13+3=16层。每层权重层都先进行卷积操作，随后使用 线性整流函数ReLU激活。池化层不涉及权重，因此不属于权重层。池化层使用2\*2池化 核，步长为2。网络末端连接一个softmax层，用于输出分类结果。此外，VGG网络还具 有如下一些特点：

（1）**结构整洁，修改方便。**该点体现在VGG网络全部使用大小统一的卷积核（3\*3）和

池化核（2\*2）。

（2）**卷积核小，网络深。**首个引起巨大关注的卷积神经网络AlexNet使用的是7\*7卷积 核，共5个卷积层，而VGG使用的3\*3卷积核更小，卷积层数13层也更深。已有 研究表明，使用多个较小卷积核的卷积层既能够减少参数，也能够提高网络的拟合 能[力［26］。](#bookmark207)

（3）**随机失活正则化（Dropout）。**Dropout方法是对某层中每个神经元设计一个消除概 率，能够在后续训练中按概率使部分神经元无效化，这样该部分神经元就不能继续 向后层输出，以达到提高神经网络泛化能力的效果。Dropout操作如图5.5所示。 VGG模型的前两个全连接层后都续接了 Dropout操作。

![](media/2eb0f3a3c38fd6a04c3de353f4203a05.jpeg)

图5.5 Dropout操作示意图

5.3.2实验结果及分析

本节实验将使用VGG16卷积神经网络在原始RSCBD数据集及经过本章所提出算法优 化处理后的RSCBD数据集（以下简称为Alpha-RSCBD）上进行对比实验，对比两种方法 对学生课堂行为判定分类的准确度。两种方法都使用VGG16网络搭建分类器，预训练模 型使用vgg16-397923af.pth，其他网络训练配置如表5.1所示：

表5.1 网络训练配置表

| 配置           | 值       |
|----------------|----------|
| 图片缩放大小   | 224\*224 |
| 学习器         | Sgd      |
| 动量           | 0.9      |
| 权重衰减       | 5e-4     |
| 初始步长       | 1e-2     |
| 最小步长       | 1e-4     |
| 学习率下降方式 | Cos      |
| 世代数         | 100      |

对于VGG16神经网络在两种数据集上的训练及验证结果，取验证集最高准确率进行

对比，两次实验的混淆矩阵如表5.2和5.3所示：

表5.2 RSCBD数据集实验混淆矩阵

| 预测行为 - | 真实行为 |      |      |      |
|------------|----------|------|------|------|
|            | 听课     | 低头 | 举手 | 起立 |
| 听课       | 227      | 4    | 6    | 2    |
| 低头       | 19       | 62   | 0    | 1    |
| 举手       | 28       | 7    | 47   | 4    |
| 起立       | 14       | 11   | 2    | 22   |

表5. 3 Alpha-RSCBD数据集实验混淆矩阵

真实行为

| 预测行为 - | 听课 | 低头 | 举手 | 起立 |
|------------|------|------|------|------|
| 听课       | 229  | 5    | 3    | 2    |
| 低头       | 4    | 77   | 0    | 1    |
| 举手       | 3    | 0    | 83   | 0    |
| 起立       | 0    | 0    | 0    | 49   |

将4种学生课堂行为判定的精确率绘制成柱状图进行对比与分析，实验结果如图5.6 所示。

120

■ Alpha-RSCBD ■ RSCBD

![](media/085f6aa0281699b38e54c2ad9d522c26.jpeg)

图5.6 识别精确率对比

实验结果显示，VGG16网络在RSCBD验证集上的最高准确率为78.51%，而在 Alpha-RSCBD验证集上的最高准确率达到了 96.05%，准确率有明显提高。

根据表5.2结果分析可知，仅使用卷积神经网络处理学生课堂行为图像的方法表现并 不稳定，各个行为识别精确率差距较大。例如，对“举手”行为的识别精确率可以达到85.45%， 而对“低头”行为的识别精确率只有73.81%，相差11.64%。本研究经深入分析认为，在 真实的课堂环境下，“举手”主要靠识别手部局部特征进行判别，属于图像特征较为明显 的学生课堂行为，因此识别准确率相对较高。而在视频图像拍摄清晰度不一的情况下，“听 课”、“低头”与“起立”特征相对模糊，且易受学生体型、环境光线、衣服颜色等因素干 扰，识别准确率较低。

而表5.3结果显示，将数据集按本章介绍的方法处理后再进行识别，对每种行为的判 定都高于对原数据集的实验结果：“听课”行为识别精确率提高18.21%，“低头”行为识别 精确率提高20.09%，“举手”行为识别精确率提高11.06%，“起立”行为识别精确率提高

18.37%。且整体识别稳定性更好，最高与最低精确率差值仅为3.13%。

5.4本章小结

本章首先介绍了基于人体姿态识别的学生课堂行为判定算法的流程，其次对基于 Alphapose 的人体骨架提取及去冗余方法进行了阐述，最后介绍了经典卷积神经网络 VGG16,并在经过处理前后的RSCBD数据集上设计了对比实验，实验结果表明本章提出 的算法能够有效地去除学生体型、环境光线、衣服颜色等冗余因素影响，突出学生骨架关 键特征，证明了本研究提出的基于人体姿态识别进行学生课堂行为判定的算法具有较好的 效果。

第六章课堂质量评估指标及计算实现

本章研究学生这一课堂教学中的重要主体，依托本研究提出的学生课堂行为识别算法， 分析学生行为在课堂质量评估中能够运用的指标，并实现计算。

1.  需求分析及指标设计

    学生作为课堂教学的主体之一，其行为能够在一定程度上评估课堂质量的好坏。从学 生课堂行为的维度来对课堂质量进行评估，主要包括以下两方面：

2.  学习状态。依靠学生的听课专注度进行衡量，能够由此进一步分析学生对该堂课程 内容是否有热情，是否能够认真思考教师讲述的内容等。
3.  互动状态。依靠学生的互动行为频率进行衡量，能够由此进一步分析学生是否能够 轻松的进入交流讨论状态，是否能够提出有创造力的想法。还能够更深入地结合教 师为主体的课堂质量评估体系，分析教师是否能够有效的引导学生思考，是否注重 学生用所学知识解决问题的能力等。

    结合第五章所提出算法能够判定的学生课堂行为（听课、低头、举手、起立），以学 习状态、互动状态为评价目标，本研究设计了抬头率、低头率、活跃度与举手次数4项课 堂质量评估指标，如表6.1所示：

    表6.1课堂质量评估指标表

| 行为状态表示 | 指标设计         |
|--------------|------------------|
| 学习状态     | 抬头率、低头率   |
| 互动状态     | 活跃度、举手次数 |

本节设计的各项指标具体解释如下：

**低头率：**当前视频图像帧中，学生课堂行为类别判定为“低头”的目标个数占检测到 总目标个数的百分比。

**抬头率：**当前视频图像帧中，学生课堂行为类别判定为“听课”、“举手”和“起立” 的目标总个数占检测到总目标个数的百分比。

**活跃度：**在所有视频图像测试帧中，存在师生互动行为“举手”和“起立”的测试帧 数所占百分比。

**举手次数：**整段测试视频中，所有学生的举手次数总计。

1.  开发软硬件环境

    本研究开发的系统用于进行第五章提出的学生课堂行为判定算法，以及本章提出的课 堂质量评估指标的计算。开发语言使用Python3.9.9,深度学习框架为具体开发及运行软硬 件环境如表6.2和6.3所示：

表6. 2本研究开发硬件环境表

硬件 配置

处理器 AMD Ryzen 7 5800H with Radeon Graphics 3.20 GHz

显卡 NVIDIA GeForce RTX 3060 Laptop GPU

显存 6GB

内存 16GB

|              | 表6. 3本研究开发软件环境表         |
|--------------|------------------------------------|
| 软件         | 配置                               |
| 操作系统     | Windows 11                         |
| 编程语言     | Python 3.9.9                       |
| 编程平台     | Pycharm Community Edition          |
| 深度学习框架 | Pytorch 1.10.2                     |
| 依赖库       | OpenCV、PIL、Numpy、CUDA、cuDNN 等 |

1.  开发思路

    本系统结合YOLO V5目标检测算法与Deepsort多目标跟踪算法，实现对视频图像帧 中学生的检测和跟踪，生成预测框并为每个预测目标赋予ID号。截取预测框中的目标后 利用Alphapose进行人体骨架提取并去冗余处理，将结果输入到卷积神经网络分类器中进 行行为判定。

    考虑到学生课堂行为具有一定的延续性，即“举手”、“起立”等动作一般持续时间大 于1秒；且视频处理速度不宜过慢，本系统不会对每一帧视频进行处理。由于采集的视频 fps都为30，本系统每10帧提取1帧，即每1秒提取3帧进行学生课堂行为判定和实时指 标计算。各课堂学生行为指标计算方式如图6.1所示。

![](media/36692a5f92daaa83d85a3fe04805d297.jpeg)

图6.1 课堂学生行为指标计算方式

1.  低头率。当前测试帧中，行为判定结果为“低头”的检测框数目占总检测框数目的 百分比。（例如，设画面中共生成m个有效检测框，其中n个有效检测框的类别为 “低头”，则低头率为n/m，转化为百分数显示。）
2.  抬头率。当前测试帧中，行为判定结果为“听课”、“举手”和“起立”的检测框数 目占总检测框数目的百分比。实际计算方式为（100% - 低头率）。
3.  活跃度。在整段视频处理过程中，含有互动行为的测试帧数占总测试帧数的百分比。

    （例如，整段视频共取测试帧m帧，其中含有“举手”、“起立”行为的有n帧，则 活跃度为n/m，转化为百分数显示。）

    （4）举手次数。记录同一 ID的预测框的连续三次分类结果序列，若分类结果序列为“非 举手”、“举手”、“举手”，则进行举手计数1次。

    1.  运行结果及分析

使用本系统对一段真实课堂视频进行检测。程序会在视频左上角实时输出“抬头率”、 “低头率”和检测到的学生人数数据。对于每个检测到的学生目标，展示其预测框、ID和 课堂行为判定信息。

针对不同类别的行为，本系统用不同颜色的预测框对目标进行展示：“听课”行为使 用红色预测框标记，“低头”行为使用黑色预测框标记，“举手”和“起立”行为使用绿色 预测框标记。

视频处理结束后，系统将输出从整段视频识别结果中计算出的“活跃度”和“举手次 数”指标。实时检测效果如图6.2所示，系统输出结果如图6.3所示。

![](media/d5e91ee7819aceac5f1e87e761268102.jpeg)

active_rate:65.79%

hand_num:2

Process finished with exit code 0

图6.3 系统输出结果

系统最终输出活跃度为65.79%，与人工观察情况基本相符；举手数输出为2次，与人 工观察情况一致。测试结果表明，本系统能够对前排学生的课堂行为有较好的识别效果， 而对后排同学的识别效果受视频的清晰度、学生被前排同学遮挡的程度影响，存在少量的 错误情况。总体上，本系统能够完成对学生课堂行为的判定和课堂质量评价指标的计算。 6.5本章小结

本章以学生为研究主体，探究其课堂行为在课堂质量评估中的作用。根据本文提出算 法能识别出的4中学生课堂行为，设计了4项课堂质量评估指标，并在真实课堂视频中进 行了测试，测试结果表明，本系统能够实现对设计指标的计算。

东南大学本科毕业设计（论文）

**第七章总结与展望**

本章将先对本文所做的所有工作进行总结，再针对不足和需要改进的方面进行阐述。

1.  工作总结

传统课堂教学中仅能人工观察进行学生课堂行为的判定，本研究结合多目标检测及跟 踪技术、人体姿态识别技术和卷积神经网络，设计算法并实现了对学生课堂行为的智能化 判定，并基于此设计课堂质量评价指标，将其融入课堂质量评估体系。本文完成的主要工 作内容如下：

1.  构建了真实环境下的学生课堂行为数据集。针对缺少数据集的问题，从互联网上采 集公开课视频，选取 4种典型的学生课堂行为：“听课”、“低头”、“举手”和“起 立”，对视频图像进行了截取和标注，制作成数据集供后续实验和开发使用。
2.  设计并实现了基于人体姿态识别的学生课堂行为判定算法。研究并对比了两种典型 的目标检测模型YOLO V5和Faster R-CNN,通过在VOC数据集上进行实验，选择 了检测准确率更高的YOLO V5搭配Deepsort目标跟踪算法作为程序搭载的模型。 对比了两种典型的人体姿态识别模型Openpose和Alphapose，并选择基于Alphapose 进行了算法的设计与实现，通过对比实验证明了本研究提出的算法的优越性。
3.  设计了基于学生课堂行为判定的课堂质量评估指标，并实现了对其的计算。根据算 法判定的4种学生课堂行为，考虑学生在课堂上的学习状态和互动状态，设计了抬 头率、低头率、活跃度和举手次数4项课堂质量评价指标，并设计程序实现了相关 计算。
    1.  工作展望

本文构建了真实环境下的学生课堂行为数据集，提出并实现了基于人体姿态识别的学 生课堂行为判定算法，通过对比实验证明了本文提出算法的有效性。此外，基于本算法实 现了以学生为主体的部分课堂质量评估指标的设计和计算。但本研究仍存在一些不足和需 要改进的方面：

1.  学生课堂行为判定较为宽泛，类别较少。本研究总结了“听课”、“低头”、“举手”、 “起立”四类学生课堂行为，但真实课堂中也可能出现“左顾右盼”的头部动作， 且“低头”动作也可进一步展开，划分出“记笔记”、“玩手机”等更加细致的课堂 行为。要实现更加细致的划分，可能需要进一步引入手部物体检测算法，有待后续

深入研究，使得行为判定更加严谨、具体。

1.  数据集规模较小。本研究所构建的真实环境下的学生课堂行为数据集仅包含 3532 张图像，其中训练集3076张，验证集456张，总体上，尤其是验证集规模较小， 还存在各类别实例数量不平衡等问题，并不算是一个完备的数据集。未来可以收集 更多的真实课堂视频，对本数据集进行丰富，作为后续研究的基础。
2.  课堂质量评价指标计算速度较慢。理想状态下，本算法希望能够在实时监控环境下 运行，以达到实时对学生行为进行检测和评价课堂质量的目的。但由于YOLO V5 和VGG16网络模型加载和调用都需要一定的响应时间，因此很难做到在速度和准 确率上达到平衡。未来可以对YOLO V5目标检测模型进行剪枝处理，以及在不大 幅影响识别准确率的情况下选择更轻量级的神经网络分类器模型，以提高算法的运 行速度。

参考文献

[1]常广建，刘亚卓，袁少伟.人工智能在学生课堂行为特征分析中的应用与设计J].电脑迷, 2018(9):1.

[2]傅德荣.教育信息处理(第2版)[M].北京师范大学出版社,2011.

1.  Flanders N A. Analyzing Teacher-Behavior[J]. Educational leadership, 1961, 19(3): 173-&.
2.  Rush S C. Transana Qualitative Video and Audio Analysis Software as a Tool for Teaching Intellectual Assessment Skills to Graduate Psychology Students[J]. Journal of Educational Technology Systems, 2014, 43(1): 55-74.
3.  Sun B, Wu Y, Zhao K, et al. Student Class Behavior Dataset: A video dataset for recognizing, detecting, and captioning students’ behaviors in classroom scenes[J]. Neural Computing and Applications, 2021, 33(14): 8335-8354.

[6]周叶.基于Faster R-CNN的小学生课堂行为检测研究卬].四川师范大学,2021.

[7]谭斌，杨书焓.基于FasterR-CNN的学生课堂行为检测算法研究臼.现代计算机,2018 (22): 45-47.

[8]蒋沁沂，张译文，谭思琪等.基于残差网络的学生课堂行为识别J].现代计算机,2019 (20): 23-27.

[9]廖鹏，刘宸铭，苏航等.基于深度学习的学生课堂异常行为检测与分析系统J].电子世界，2018 (8): 97-98.

[10]朱朝.基于深度学习的课堂低头抬头行为状态识别与应用[D].华中师范大学,2019.

[11]徐家臻，邓伟，魏艳涛.基于人体骨架信息提取的学生课堂行为自动识别J].现代教育技术,2020.

[12]杨凡.基于人体骨架和深度学习的学生课堂行为识别研究卬].武汉：华中师范大学,2020.

[13]黄冠.基于深度学习的学生课堂行为识别研究卬].中国矿业大学,2021.

[14]高科威.基于卷积神经网络的课堂人体行为识别研究[0.太原理工大学,2020.

[15]林灿然.基于深度学习的课堂学生行为识别技术研究与分析系统设计[D].广东工业大学,2020.

1.  Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.
2.  Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//European conference on computer vision. Springer, Cham, 2016: 21-37.
3.  Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern

recognition. 2014: 580-587.

1.  Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448.
2.  Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[J]. Advances in neural information processing systems, 2015, 28.
3.  Bewley A, Ge Z, Ott L, et al. Simple online and realtime tracking[C]//2016 IEEE international conference on image processing (ICIP). IEEE, 2016: 3464-3468.
4.  Zhang Y, Wang C, Wang X, et al. A simple baseline for multi-object tracking[J]. arXiv preprint arXiv:2004.01888, 2020, 3(4): 6.
5.  Wojke N, Bewley A, Paulus D. Simple online and realtime tracking with a deep association metric[C]//2017 IEEE international conference on image processing (ICIP). IEEE, 2017: 3645-3649.
6.  Cao Z, Simon T, Wei S E, et al. Realtime multi-person 2d pose estimation using part affinity fields[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7291-7299.
7.  Fang H S, Xie S, Tai Y W, et al. Rmpe: Regional multi-person pose estimation[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2334-2343.

    [26]杨明健，黎镜林，郭锐坤，等.基于OpenPose的人体睡姿识别实现与研究臼.物理实验，2019, 39(8): 45-49.

8.  Krizhevsky A , Sutskever I , Hinton G . ImageNet Classification with Deep Convolutional Neural Networks[J]. Advances in neural information processing systems, 2012, 25(2).
9.  MD Zeiler, Fergus R . Visualizing and Understanding Convolutional Networks[J]. Springer, Cham, 2014.
10. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.
11. Szegedy C , Liu W , Jia Y , et al. Going Deeper with Convolutions[J]. IEEE Computer Society, 2014.
12. He K , Zhang X , Ren S , et al. Deep Residual Learning for Image Recognition[J]. IEEE, 2016.
13. Newell A, Yang K, Deng J. Stacked hourglass networks for human pose estimation[C]//European conference on computer vision. Springer, Cham, 2016: 483-499.

[33]毛梦蝶.基于深度哈希网络的图像检索研究[0.电子科技大学,2018.

东南大学本科毕业设计（论文）

**致 谢**

转眼间，四年的大学本科生涯即将步入终章，历时数月的毕业论文也即将收尾。在这 里，对所有帮助过我的人表示感谢：

首先要感谢我的指导老师陶军老师。在这半年的毕业设计及论文撰写工作中，陶老师 时刻关注着我的进度情况，无论在宏观的研究方向还是细节的实现瓶颈上，都给到了我很 大的帮助，带我走进了科学研究的大门。同时陶老师严谨治学的态度以及孜孜不倦的科研 精神也让我受到了很大的鼓舞，在此，向陶老师表示由衷的感谢。希望能够在研究生阶段 继续向陶老师学习，不断提升自我。

其次要感谢培养我四年的东南大学。这四年间，我在努力学习和生活中探寻校训“止 于至善”的内涵。在九龙湖这片土地上，我在汲取了许多知识的同时，也幸运地收获了一 些良师、益友。在即将毕业之际，我想我对“止于至善”，应该也有了更深层次的理解。

感谢我的父母，养育之恩，无以为报。是你们的无私奉献，让我有机会走在求学的道 路上；而你们的谆谆教诲、以身作则，则让我得以前进至今，顺利结束大学本科生涯。父 亲的“全力以赴，不留遗憾”，母亲的“好好吃饭，早点休息”每天都萦绕在我的耳边， 成为支撑我克服困难、不断进步的动力。

另外还要感谢一同进行毕业设计研究的同学和舍友，在各自的研究之余能够互相帮助， 共同解决一些难题，在焦虑的时候互相鼓励，消减了研究过程中的很多枯燥和烦恼。

最后，向百忙之中参与本文评审的老师们表示感谢，老师们的宝贵意见将指引我未来 继续前进的方向。
